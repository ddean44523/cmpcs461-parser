{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab21ef94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "id": "40b42a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# author: Dan Dean, 966066212\n",
    "import sys\n",
    "LETTERS = \"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "DIGITS = \"0123456789\"\n",
    "METACHARS = \"</bodyuliEOI\" #we don't want the > so that we dont accidently take \n",
    "TAGS = [\"<b>\",\"<i>\",\"<body>\",\"<li>\",\"<ul>\"]\n",
    "ENDTAGS = [\"</b>\",\"</i>\",\"</body>\",\"</li>\",\"</ul>\",\"<EOI>\"]\n",
    "TAGSMATCH = {\n",
    "    \"<b>\": \"</b>\"\n",
    "    ,\"<i>\" :\"</i>\"\n",
    "    ,\"<body>\":\"</body>\"\n",
    "    ,\"<li>\":\"</li>\"\n",
    "    ,\"<ul>\" : \"</ul>\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "b8a02651",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parser:\n",
    "    def __init__(self, s):\n",
    "        self.lexer = Lexer(s+\"$\")\n",
    "        self.token = self.lexer.nextToken()\n",
    "        self.level = 0\n",
    "    \n",
    "    def toList(self):\n",
    "        words = []\n",
    "        alexis = Lexer(s)\n",
    "        while True:\n",
    "            tok = alexis.nextToken()\n",
    "            words.append(tok)\n",
    "            if tok == \"<EOI>\":\n",
    "                break\n",
    "        return words\n",
    "        \n",
    "    def run(self):\n",
    "        self.statement()\n",
    "        \n",
    "    def statement(self):\n",
    "        print(\"<Statement>\")\n",
    "        self.assignmentStmt()\n",
    "        while self.token.getTokenType() == SEMICOLON:\n",
    "            print(\"\\t<Semicolon>;</Semicolon>\")\n",
    "            self.token = self.lexer.nextToken()\n",
    "            self.assignmentStmt()\n",
    "        self.match(EOI)\n",
    "        print(\"</Statement>\")\n",
    "        \n",
    "    def assignmentStmt(self):\n",
    "        print(\"\\t<Assignment>\")\n",
    "        val = self.match(ID)\n",
    "        print(\"\\t\\t<Identifier>\" + val + \"</Identifier>\")\n",
    "        self.match(ASSIGNMENTOP)\n",
    "        print(\"\\t\\t<AssignmentOp>:=</AssignmentOp>\")\n",
    "        self.expression()\n",
    "        print(\"\\t</Assignment>\")\n",
    "        \n",
    "    def expression(self):\n",
    "        if self.token.getTokenType() == ID:\n",
    "            print(\"\\t\\t<Identifier>\" + self.token.getTokenValue() \\\n",
    "                   + \"</Identifier>\")\n",
    "        elif self.token.getTokenType() == INT:\n",
    "            print(\"\\t\\t<Int>\" + self.token.getTokenValue() + \"</Int>\")\n",
    "        elif self.token.getTokenType() == FLOAT:\n",
    "            print(\"\\t\\t<Float>\" + self.token.getTokenValue() + \"</Float>\")\n",
    "        else:\n",
    "            print(\"Syntax error: expecting an ID, an int, or a float\" \\\n",
    "                  + \"; saw:\" \\\n",
    "                  + typeToString(self.token.getTokenType()))\n",
    "            sys.exit(1)\n",
    "        self.token = self.lexer.nextToken()\n",
    "        \n",
    "    def match(self, tp):\n",
    "        val = self.token.getTokenValue()\n",
    "        if (self.token.getTokenType() == tp):\n",
    "            self.token = self.lexer.nextToken()\n",
    "        else: self.error(tp)\n",
    "        return val\n",
    "    \n",
    "    def error(self, tp):\n",
    "        print (\"Syntax error: expecting: \" + typeToString(tp) \\\n",
    "               + \"; saw: \" + typeToString(self.token.getTokenType()))\n",
    "        sys.exit(1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "f51d0657",
   "metadata": {},
   "outputs": [],
   "source": [
    "def typeToString (tp):\n",
    "    if (tp == INT): return \"Int\"\n",
    "    elif (tp == FLOAT): return \"Float\"\n",
    "    elif (tp == ID): return \"ID\"\n",
    "    elif (tp == SEMICOLON): return \"Semicolon\"\n",
    "    elif (tp == ASSIGNMENTOP): return \"AssignmentOp\"\n",
    "    elif (tp == EOI): return \"EOI\"\n",
    "    return \"Invalid\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "id": "fbd2dc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lexer:\n",
    "    # stmt is the current statement to perform the lexing;\n",
    "    # index is the index of the next char in the statement\n",
    "    def __init__ (self, s):\n",
    "        self.stmt = s + \"<EOI>\"\n",
    "        self.index = 0\n",
    "        self.nextChar()\n",
    "        \n",
    "    def nextToken(self):\n",
    "        while 1:\n",
    "            if self.checkChar(\"<\"):\n",
    "                return (self.consumeChars(METACHARS))\n",
    "            elif self.ch in LETTERS+DIGITS:\n",
    "                return (self.consumeChars(LETTERS+DIGITS))\n",
    "            else:\n",
    "                self.nextChar()\n",
    "            \n",
    "    def nextChar(self):\n",
    "        self.ch = self.stmt[self.index] \n",
    "        self.index = self.index + 1\n",
    "        if self.index >= len(self.stmt):\n",
    "            self.index = 0\n",
    "            \n",
    "    def consumeChars (self, charSet):\n",
    "        r = self.ch\n",
    "        self.nextChar()\n",
    "        while (self.ch in charSet):\n",
    "            r = r + self.ch\n",
    "            self.nextChar()\n",
    "        if charSet == METACHARS:\n",
    "            r += \">\"\n",
    "            if self.index < len(self.stmt):\n",
    "                self.nextChar()\n",
    "        return r\n",
    "    \n",
    "    def checkChar(self, c):\n",
    "        if (self.ch==c):\n",
    "            return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "id": "363b46c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Token:\n",
    "    \"A class for representing Tokens\"\n",
    "    def __init__ (self, s):\n",
    "        self.stmt = s\n",
    "        self.type = \"INVALID\"\n",
    "        self.type = self.typer()\n",
    "        self.level = 0\n",
    "        \n",
    "    def getToken(self):\n",
    "        return self.stmt\n",
    "    \n",
    "    def getType(self):\n",
    "        return self.type\n",
    "    \n",
    "    def typer(self):\n",
    "        if self.isTag():\n",
    "            if self.stmt in ENDTAGS:\n",
    "                if self.stmt == \"<EOI>\":\n",
    "                    return \"EOI\"\n",
    "                return  \"ENDTAG\"\n",
    "            else:\n",
    "                return  \"TAG\"\n",
    "        else:\n",
    "            for i in self.stmt:\n",
    "                if i not in LETTERS+DIGITS:\n",
    "                    return \"INVALID\"\n",
    "            return \"TEXT\"\n",
    "    \n",
    "    def isTag(self):\n",
    "        return self.stmt in TAGS + ENDTAGS\n",
    "    \n",
    "    def isType(self, check):\n",
    "        if self.type == check: return True\n",
    "        return False\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.type + \" token: \" + self.stmt \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "id": "0bebd2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParserTwo:\n",
    "    \"\"\"| WEBPAGE -> <body> { TEXT } </body>\n",
    "       | TEXT -> STRING | <b> TEXT </b> | <i> TEXT </i> | <ul> { LISTITEM } </ul>\n",
    "       | LISTITEM -> <li> TEXT </li>\n",
    "       | STRING -> LETTER | DIGIT    \"\"\"\n",
    "    def __init__(self, s):\n",
    "        self.lexer = Lexer(s)\n",
    "        self.tokenList = self.toList()\n",
    "        self.token = self.tokenList[0]\n",
    "        self.cur_level = 0\n",
    "        \n",
    "    def toList(self):\n",
    "        words = []\n",
    "        while True:\n",
    "            tok = self.lexer.nextToken()\n",
    "            words.append(Token(tok))\n",
    "            if tok == \"<EOI>\":\n",
    "                break\n",
    "        return words\n",
    "        \n",
    "    def parse(self):\n",
    "        last_tag = self.tokeList[0]\n",
    "        for tok in self.tokenList:\n",
    "            if tok.getType() == \"TAG\":\n",
    "                print(\"   \"*self.cur_level + tok.getToken())\n",
    "                self.cur_level +=1\n",
    "                last_tag = tok\n",
    "            elif tok.getType() == \"ENDTAG\":\n",
    "                \n",
    "                self.cur_level -= 1\n",
    "                print(\"   \"*self.cur_level + tok.getToken())\n",
    "            elif tok.getType() == \"EOI\":\n",
    "                print(\"done\")\n",
    "            else:\n",
    "                print(\"   \"*self.cur_level + tok.getToken())\n",
    "                \n",
    "    def run(self):\n",
    "        self.parse()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "id": "cbb74451",
   "metadata": {},
   "outputs": [],
   "source": [
    "PT = ParserTwo((\"<body> google <b><i><ul> yahoo</ul></i></b></body>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "id": "6a7c90ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<body>\n",
      "   google\n",
      "   <b>\n",
      "      <i>\n",
      "         <ul>\n",
      "            yahoo\n",
      "         </ul>\n",
      "      </i>\n",
      "   </b>\n",
      "</body>\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "PT.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "id": "ed20c9f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TAG token: <body>,\n",
       " TEXT token: google,\n",
       " TAG token: <b>,\n",
       " TAG token: <i>,\n",
       " TAG token: <ul>,\n",
       " TEXT token: yahoo,\n",
       " ENDTAG token: </ul>,\n",
       " ENDTAG token: </i>,\n",
       " ENDTAG token: </b>,\n",
       " ENDTAG token: </body>,\n",
       " EOI token: <EOI>]"
      ]
     },
     "execution_count": 498,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PT.tokenList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "id": "32f8e288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENDTAG token: </ul> -> <ul>\n",
      "ENDTAG token: </i> -> <i>\n",
      "ENDTAG token: </b> -> <b>\n",
      "ENDTAG token: </body> -> <body>\n"
     ]
    }
   ],
   "source": [
    "stack = []\n",
    "for tok in PT.tokenList: \n",
    "    if tok.isType(\"TAG\") :\n",
    "        stack.append(tok)\n",
    "    if tok.isType(\"ENDTAG\"):\n",
    "        compIndex = ENDTAGS.index(tok.getToken())\n",
    "        print(tok,\"->\",TAGS[compIndex])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "id": "cfa8f13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "stack = [1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "id": "f3378948",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 486,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stack.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a02d533",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
